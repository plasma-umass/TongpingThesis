\label{sec:evaluation}

We perform our evaluations on a quiescent 8-core system (dual
processor with 4 cores), with 8GB of RAM. Each processor is a 4-core 64-bit Intel Xeon, running at 2.33 Ghz with a 4MB L2 cache. For compatibility reasons, we compiled all applications to a 32-bit target using GCC. All performance data is the average of 10 runs, excluding the maximum and minimum values.

The evaluation answers the following questions:

\begin{itemize}
\item How effective is \sheriffdetect{} at finding false sharing and guiding programmers to their resolution? (Section~\ref{sec:effecteval})
\item What is \sheriffdetect{}'s performance overhead? (Section~\ref{})
\item How sensitive is \sheriffdetect{} to different sampling rate? (Section~\ref{}) 
\item How effective does \sheriffprotect{} mitigate false sharing? (Section~\ref{})
\end{itemize}

\subsection{\sheriffdetect{} Effectiveness}

\label{sec:effecteval}

This section evaluates whether \sheriffdetect{} can be used to find false sharing problems, both in synthetic test cases and in actual applications.

We developed a range of microbenchmarks that exemplify different situations related to false sharing. We evaluate these benchmarks on both \SheriffDetect{} and Intel's Performance Tuning Utility(PTU v3.2), the previous state-of-the-art work of false sharing detection.
Detection results are shown in Table~\ref{table:microbenchmarks}. \sheriffdetect{} only reports those false sharing instances that can possibly affect the performance, while correctly ignores those cases without no performance impact.
PTU has false alarms/positives.  It does not track those pattern of accesses, which reports false positives for those non-interleaved accesses. Also, PTU does not track memory deallocations, thus it can not filter out those pseudo false sharing caused by memory reuse. \sheriffdetect{} avoids all of these problems and reports false sharing problems correctly. 


\begin{table}
\centering
\begin{tabular}{l|l|l|l}
\hline
{\bf \small Microbenchmark} & {\bf \small Perf Sensitive } & {\bf \small \sheriffdetect{} } & {\bf \small PTU } \\
\hline

\small \textbf{False Sharing (adjacent objects)} & YES & \cmark{} & \cmark{} \\
\small \textbf{False Sharing (same object)} & YES & \cmark{} & \cmark{} \\
\hline
\small \textbf{True Sharing} & NO & & \\
\small \textbf{Non-interleaved False Sharing} & NO & & \xmark{}\\
\small \textbf{Heap Reuse(no sharing)} & NO & & \xmark{}\\
\hline
\end{tabular}
\caption{False sharing detection results using PTU and \sheriffdetect{}. \sheriffdetect{} correctly reports only actual false sharing instances, with a performance impact;
\cmark{} indicates a correct report and \xmark{} indicates a false alarm. 
\label{table:microbenchmarks}}
\end{table}

We further evaluate \SheriffDetect{} and PTU on two widely-used benchmarks suites, Phoenix~\cite{phoenix-hpca} and PARSEC~\cite{parsec}. We use the simlarge inputs for all applications of PARSEC. For Phoenix, we chose available parameters that allow the programs to run as long as possible. As of this writing, we were unable to successfully
compile \texttt{raytrace} and \texttt{vips}, and \sheriff{} is
currently unable to run \texttt{x264}, \texttt{bodytrack},
and \texttt{facesim}. Freqine currently can not support pthreads. Thus, those benchmarks are excluded here. 
 
\begin{table}
\centering
\begin{tabular}{l|r|r}
\hline
{\bf \small Benchmark} & {\bf \small PTU} & {\bf \small \sheriffdetect{}}\\
 & {\# Lines} & {\# Objects}\\
\hline
\small \textbf{kmeans} & 1916 &  2 \\
\small \textbf{linear\_regression} & 5 & 1 \\
\small \textbf{matrix\_multiply} & 468 & 0\\
\small \textbf{pca} & 45 & 0 \\
\small \textbf{reverseindex} & N/A & 5 \\
\small \textbf{word\_count} & 4 & 3\\
\hline
\small \textbf{canneal} & 1 & 1 \\
\small \textbf{fluidanimate} & 3 & 1 \\
\small \textbf{streamcluster} & 9 & 1\\
\small \textbf{swaptions} & 196 & 0\\
\hline
\small \textbf{Total} & 2647 & 14\\
\hline
\end{tabular}
\caption{Overall detection results of PTU and \sheriffdetect{} on Phoenix and PARSEC benchmark suites. We only list those benchmarks that at least one of tools reports false sharing problems. For PTU, we show how many cache lines are marked as falsely shared. For \sheriffdetect{}, we show how many objects are reported by \sheriffdetect{} (with cache invalidations larger than 100). The item marked as ``N/A'' means PTU fails to show results because it runs out of memory.
\label{table:fsdetection}}
\end{table}


The overall results are shown in Table~\ref{table:fsdetection}. PTU reports that 2647 cache lines may exist false sharing problems, given that they can report false positives. \sheriffdetect{} reveals that seven out of sixteen evaluated benchmarks have some false sharing issues. Totally, only 14 objects are reported, but only 4 of them shows a big number of cache invalidations. 

Several reasons contributes to this big difference. First, PTU reports cache lines information about false sharing objects, while \SheriffDetect{} only reports objects. Second, PTU reports multiple times if a heap object, with the same allocation site, is allocated multiple times. 
Third, PTU may report false positives since it do not track interleaved accesses and overrate the problems caused by heap reuses. 


We manually fix these four false sharing problems based on reports of \SheriffDetect{}, and showed the performance data in Table~\ref{table:perfafterfix}. To explain why performance improvement are different, we examine the maximum possible updates occurred on these false sharing objects, the reason of performance improvement. For example, \texttt{linear\_regression} has the largest updates, thus causes serious performance problem because of false sharing. 

\begin{table}
\centering
\begin{tabular}{l|r|r}
\hline
{\bf \small Benchmark} & {\bf \small Performance Improvement} & {\bf \small Updates}\\
 & & (M)\\
\hline
\small \textbf{linear\_regression} & 818\% & 1323.6\\
\small \textbf{reverseindex} &  2.4\% & 0.4\\
\small \textbf{streamcluster} & 5.4\% & 28.7\\
\small \textbf{word\_count} &  1\% & 0.3\\
\hline
\end{tabular}
\caption{Performance data for four false sharing benchmarks. 
All data are obtained using the standard \pthreads{} library. 
``Updates'' shows how many million updates (in total) occurred on falsely-shared cache lines.
\label{table:perfafterfix}}
\end{table}


In \texttt{reverse\_index} and \texttt{word\_count}, multiple threads repeatedly modify the same heap object. The pseudo code for these two benchmarks are listed in Figure~\ref{fig:reverseindex}. We may use thread-local copy to avoid the false sharing problem here; each thread can modify a temporary variable first and then modify the global \texttt{use\_len} in the end of thread.

\begin{figure}[!t]
\begin{lstlisting}
int * use_len;
void insert_sorted(int curr_thread) {
   ......	
   // After finding a new link
   (use_len[curr_thread])++;
   ......	
}
\end{lstlisting}
\caption{A fragment of source code from \texttt{reverse\_index}. False sharing arises when adjacent threads 
modify the \texttt{use\_len} array. 
\label{fig:reverseindex}}
\end{figure}

\texttt{Linear\_regression}'s false sharing problem is a little different (see Figure~\ref{fig:linear_regression}). 
Two different threads write to the same cache line when the
structure \texttt{lreg\_args} is not cache line aligned. This problem can be avoided easily by padding the structure \texttt{lreg\_args}.

\begin{figure}[!t]
\begin{lstlisting}
struct {
  long long SX;
  long long SY;
  long long SXX;
  ......
} lreg_args;

void *lreg_thread(void *args_in) {
  struct lreg_args * args = args_in;
  for(i = 0; i < args->num_elems; i++) {
    args->SX  += args->points[i].x;
    args->SXX += args->points[i].x 
   	         * args->points[i].x;
  }
  ......	
}
\end{lstlisting}
\caption{A fragment from \texttt{linear\_regression} code. 
Each thread is passed in a different address (\texttt{struct lreg\_args}) and each thread can work on its corresponding \texttt{args\_in}. 
Unfortunately, the size of \texttt{struct lreg\_args} is not cache line aligned (52 bytes) and that
causes two different threads to write to the same cache line simultaneously. 
\label{fig:linear_regression}}
\end{figure}

The false sharing problem detected in \texttt{streamcluster} (one of the PARSEC benchmarks) is similar to the false sharing problem in \texttt{linear\_regression}; two different threads are writing on the same cache line.  In fact, the author tried to avoid the false sharing problems and make every stride a multiple times of cache line size. But the default cache line size is 32 bytes, which is different from the actual physical cache line size that we are used in evaluation (64 bytes).  By simply setting the \texttt{CACHE\_LINE} macro to 64 bytes, it is possible to avoid this false sharing problem completely.


\subsection{Ease of locating false sharing problems}

\noindent
To illustrate how \sheriffdetect{} can precisely locate false sharing problems, we 
use one benchmark (\texttt{word\_count}, a PHOENIX benchmark) as
an example. Our experience with diagnosing other false sharing issues is similar.

Here is an example output from \sheriffdetect{} from \texttt{word\_count}.

\begin{verbatim} 
1st object, cache interleaving writes 
13767 times (start at 0xd5c8e140). 
Object start 0xd5c8e160, length 32. 
It is a heap object with callsite:
callsite0:./wordcount_pthreads.c:136
callsite1:./wordcount_pthreads.c:441
\end{verbatim}

Line 136 (\texttt{wordcount\_\pthreads{}.c}), 
contains the following memory allocation call:

\begin{verbatim}
use_len=malloc(num_procs*sizeof(int));
\end{verbatim}

Grepping for \texttt{use\_len}, a global pointer, quickly leads to this line:

\begin{verbatim}
use_len[thread_num]++;
\end{verbatim}

Now it is clear that different threads are modifying the same object
(use\_len). Fixing the problem by using the
thread-local data copies is now straightforward~\cite{detect:intel}.

By contrast, compare PTU's output in Figure~\ref{fig:wordcount}. Finding this problem is far more complicated with PTU, since it only presents functions using each cache line, not to mention the fact that PTU can
report huge numbers of false positives.  Another shortcoming
of PTU is that ``Collected Data Refs'' number cannot be used as a metric to evaluate the significance of false sharing problems. For this example, PTU only reports 12 references (versus 13767 times for \sheriffdetect{}).

\begin{figure*}[!t]
\centering
\includegraphics[width=6in]{sheriff/figure/wordcount}
\caption{PTU output for \texttt{word\_count}.
\label{fig:wordcount}}
\end{figure*}

%That is why we cannot relying on PTU to do the analysis of false sharing
%problems given the large number of cache lines involved. 

\subsection{\sheriffdetect{} Performance Overhead}
\label{sec:results-runtime-overhead}

\begin{figure*}[!t]
\centering
\includegraphics[width=6in]{sheriff/figure/detective.png}
\caption{\sheriffdetect{} overhead across two suites of benchmarks,
  normalized to the performance of the \pthreads{} library (lower is better). 
  With two exceptions, its overhead is acceptably low.
\label{fig:overhead}}
\end{figure*}


This section shows the runtime overhead of \sheriffdetect{} (comparing to \pthreads{})on
two multithreaded benchmarks suites, PHOENIX and PARSEC.  The results
can be seen from Figure~\ref{fig:overhead}.  

\texttt{linear\_regression} exhibits almost
a 10X speedup against the one using \pthreads{} library even with the added overhead of sampling and 
other mechanisms of \sheriffdetect{}.  There is a
serious false sharing problem inside (see
Table~\ref{table:perfafterfix}) which both \sheriffdetect{} and \sheriffprotect{} eliminate
automatically. 

There are two benchmarks on which \sheriffdetect{} do not perform well. 
One is \texttt{canneal}, the performance overhead of \sheriffdetect{}
on this benchmark is about 7X slower than the one using \pthreads{}
library. Another one is \texttt{fluidanimate}, the performance overhead is about 
14X slower than that using \pthreads{}.

According to our analysis, the transaction number and dirty pages are two main causes 
of the overhead. For most of time, more transaction number can cause more dirty pages.  
In order to find out what can affect the performance of these two benchmarks, 
we get some characteristics data(see Table~\ref{tbl:characteristics} about these 
two benchmarks when they are using 
the \sheriffdetect{}.

\begin{table}
\centering
\begin{tabular}{|l|r|r|r|}
\hline
{\bf \small Benchmark} & {\bf \small Trans} &{\bf \small DirtyPages} & {\bf \small Runtime} \\
 & {\#} & {M} & {s}\\
\hline
{\bf \small canneal} & 930 & 2.9 & 74.3 \\
{\bf \small fluidanimate} & 18696114 & 2.15 & 21.56\\
\hline
\end{tabular}
\caption{Characteristics of slower benchmarks in \sheriffdetect{}.
\label{table:characteristics}}
\end{table}

From the table, we can easily find out that these two benchmarks share the same attribute, having large amount of dirty pages. 
For one dirty page, \sheriffdetect{} need two protections, creation of the Copy-On-Write version and
different version of twin pages, checking the false sharing problem inside every periodical checking cycle and 
commits to the shared mapping. Given large amount of dirty pages, copying alone is very expensive 
since one dirty page needs at least 3 copies. 
For \texttt{fluidanimate}, 2.2 million pages needs about 6.8 million copies, which can 
acount for about 20 seconds copying overhead since copying one gigabyte of memory takes approximates 0.75 seconds.
shared pages, which leads to substantial overhead.
Examination of the source code of \texttt{fluidanimate} reveals a large number of lock
calls, \sheriff{} replaces lock calls with their interprocess variants
and triggers a transaction end and begin for each, adding some overhead if there are some shared pages.
 
\begin{comment}
The worst case for \sheriff{} is exemplified
by \texttt{ferret}, which modifies a huge number of pages (about
3.45G) and has a large number of transactions (about 1M).
We also measured charecteristics of our benchmark suites in
Table~\ref{table:characteristics}.  The
following parameters determine the performance of \sheriff{}.

\begin{itemize}
\item
Pages written: each write on a protected page imposes
additional overhead to unprotect the page in the page fault handler.
In the sampling handler, \sheriff{} must check for cache writes for
each shared written page, and at the end of transaction, \sheriff{} must
check cache writes for each page and commit the modification to the shared
space.

\item
Transaction length: \sheriff{} introduces overhead in the beginning
of transaction and in the end of each transaction. Longer transactions
amortizes this overhead.

\item 
Allocation times: \sheriff{} (in detection mode) attaches callsite information for every
allocated object, slowing allocation.

\item
Cache cleanup size: \sheriff{} cleans up the invalid cache counting
information in the memory allocation if one allocation is involving in
the re-usage of memory of those freed memory objects.
\end{itemize}

From the results from Table~\ref{table:characteristics}, we can
confirm our analysis.  Allocation times and cache cleanup size have
little impact on performance. However, when the number and rate of
pages written is large, performance suffers.

Figure~\ref{fig:overhead} shows that \sheriff{}'s overhead is highest for
the following two benchmarks: \texttt{fluidanimate} and \texttt{canneal}.
For \textt{canneal}, different threads are writting to a lot of shared pages
benchmarks \texttt{ferret}, \texttt{reverse\_index}, \texttt{dedup}
and \texttt{fluidanimate}. Characteristics showed in
Table~\ref{table:characteristics} that the first three benchmarks have
a very high rate of page updates (\textbf{PagesPerMs}). 
\texttt{fluidanimate} is an outlier if we are just using the \textbf{PagesPerMs} metrics.
The reason of \texttt{fluidanimate} has a high overhead is that there
are huge amounts of transactions inside (about 10M). Examination of
the source code revealed a large number of lock calls in this
application. \sheriff{} replaces lock calls with their interprocess
variants and triggers a transaction end and begin for each, adding
overhead.  The worst case for \sheriff{} is exemplified
by \texttt{ferret}, which modifies a huge number of pages (about
3.45G) and has a large number of transactions (about 1M).

\begin{table*}
\centering
\begin{tabular}{|l|rrrr|rr|r|}
\hline
{\bf \small Benchmark} & {\bf \small PagesWritten} & {\bf \small Commits} & {\bf Allocs} & {\bf \small CleanupSize} & {\bf \small TranLength(ms)} & {\bf \small PagesPerTran} & {\bf \small PagesPerMs}\\
\hline
\small \textbf{histogram} & 0 & 24 & 2 & 0 & 12.5 & 0 & 0\\
\small \textbf{kmeans} & 1312 & 3836 & 101002 & 0 & 4.15 & 0.34 & 0.08\\
\small \textbf{linear\_regression} & 16 & 24 & 3 & 0 & 38.6 & 0.67 & 0.02\\
\small \textbf{matrix\_multiply} & 16 & 24 & 11 & 0 & 313.23 & 0.67 & 0.0\\
\small \textbf{pca} & 0 & 47 & 2 & 0 & 450.69 & 0 & 0.0\\
\small \textbf{reverseindex} & 260201 & 156409 & 250927 & 0 & 0.05 & 1.66 & 30.99 \\
\small \textbf{string\_match} & 0 & 24 & 7 & 0 & 104.75 & 0 & 0.00\\
\small \textbf{word\_count} & 145 & 89 & 38 & 32 & 25.08 & 1.63 & 0.06\\
\hline
\small \textbf{blackscholes} & 0 & 23 & 4 & 0 & 453.51 & 0 & 0.0\\
\small \textbf{canneal} & 8 & 1056 & 5974612 & 0 & 10.32 & 0.01 & 0.0\\
\small \textbf{dedup} & 76184 & 45636 & 8291 & 0 & 0.04 & 1.67 & 44.9\\
\small \textbf{ferret} & 904381 & 1072258 & 110558 & 0 & 0.01 & 0.84 & 76.04\\
\small \textbf{fluidanimate} & 8 & 10018550 & 135430 & 352 & 0.00 & 0.00 & 0.00\\
\small \textbf{freqmine} & 0 & 1 & 33 & 0 & 11524.6 & 0 & 0.0 \\
\small \textbf{streamcluster} & 32824 & 128557 & 12 & 294 & 0.02 & 0.26 & 10.42\\
\small \textbf{swaptions} & 48 & 24 & 388 & 0 & 167.23 & 2 & 0.01\\
\hline
\end{tabular}
\caption{Characteristics of benchmarks. 
\label{table:characteristics}}
\end{table*}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%%%% Some data to list the effectiveness of this tool.
%%%%%% How many caches are carried for each test case. 
%%%%%% Whether all caches has false sharing problem.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance of \sheriffprotect{}}
\label{sec:results-runtime-overhead}

\begin{figure*}[!t]
\centering
\includegraphics[width=6in]{sheriff/figure/patrolperf.pdf}
\caption{\sheriffprotect{} performance across two suites of benchmarks,
  normalized to the performance of the \pthreads{} library (see
  Section~\ref{sec:results-runtime-overhead}). In case of
  catastrophic false sharing, \sheriffdetect{} dramatically increases performance.
\label{fig:patrol}}
\end{figure*}

Here, we examine the performance improvement by tolerating the false sharing problems in
\sheriffprotect{}.
The performance improvement can be seen in the Figure~\ref{fig:patrol}.  

From the results, we can see that \texttt{linear\_regression} exhibits almost
a 10X speedup against the one using \pthreads{} library.  
By tolerating the serious false sharing problem inside (see
Table~\ref{table:perfafterfix}), \sheriffprotect{} achieves a significant performance 
benefit for this benchmark.
\texttt{histogram} performance benefit comes from one munmap() call to unmap about 400M's file, 
we currently are not sure about why multi-process framework can perform better in this case.

There are three benchmarks which runs at most 30\% slower than using the \pthreads{}. 
We examine the reasons to cause this slowdown. 
For \texttt{kmeans}, this application creates more than 3000 threads about 8 seconds. Since the overhead
to create one process is higher than that to create one thread, this part of 
overhead dominates most of overhead. 

For \texttt{reverse\_index} and \texttt{fluidanimate}, 
they exhibit slowdown because of the use of the processes-as-threads framework. 
This performance impact arises
from the use of a file-based mapping, which connects the private
mapping and shared mapping. The Linux page fault handler does more
work when operating on file-based pages than on anonymous pages (the
normal status of heap-allocated pages). The first write on a
file-mapped page repopulates information from the file's page table
entry. Also, the shared store for all heap pages is initially set to
\texttt{MAP\_SHARED}, so writing to one shared page can cause a
Copy-On-Write operation in the kernel even when there is only one user.
\texttt{fluidanimate} has an enormous number of transactions(18 Million), \sheriffprotect{} 
introduces some additional ovherhead for every trnasaction. That also accounts for part of
overhead.
